#-
#  - Name
#  - Date Time@Location
#  - Title
#  - "url"
#  - >
#    Abstract
-
  - Johnson He
  - September 27th, 4:30-5pm@3901 BBB
  - Oxidizing OCaml with Modal Memory Management 
  - "https://dl.acm.org/doi/10.1145/3674642"
  - >
    Programmers can often improve the performance of their programs by reducing
    heap allocations: either by allocating on the stack or reusing existing
    memory in-place. However, without safety guarantees, these optimizations
    can easily lead to use-after-free errors and even type unsoundness. In this
    paper, we present a design based on modes which allows programmers to
    safely reduce allocations by using stack allocation and in-place updates of
    immutable structures. We focus on three mode axes: affinity, uniqueness and
    locality. Modes are fully backwards compatible with existing OCaml code and
    can be completely inferred. Our work makes manual memory management in
    OCaml safe and convenient and charts a path towards bringing the benefits
    of Rust to OCaml.
-
  - Andrew Blinn
  - October 18th, 4-5pm@3901 BBB
  - Statically Contextualizing Large Language Models with Typed Holes
  - "https://arxiv.org/abs/2409.00921"
  - >
    Large language models (LLMs) have reshaped the landscape of program
    synthesis. However, contemporary LLM-based code completion systems often
    hallucinate broken code because they lack appropriate context, particularly
    when working with definitions not in the training data nor near the cursor.
    This paper demonstrates that tight integration with the type and binding
    structure of a language, as exposed by its language server, can address
    this contextualization problem in a token-efficient manner. In short, we
    contend that AIs need IDEs, too! In particular, we integrate LLM code
    generation into the Hazel live program sketching environment. The Hazel
    Language Server identifies the type and typing context of the hole being
    filled, even in the presence of errors, ensuring that a meaningful program
    sketch is always available. This allows prompting with codebase-wide
    contextual information not lexically local to the cursor, nor necessarily
    in the same file, but that is likely to be semantically local to the
    developer's goal. Completions synthesized by the LLM are then iteratively
    refined via further dialog with the language server. To evaluate these
    techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web
    applications. These applications serve as challenge problems due to their
    reliance on application-specific data structures. We find that
    contextualization with type definitions is particularly impactful. After
    introducing our ideas in the context of Hazel we duplicate our techniques
    and port MVUBench to TypeScript in order to validate the applicability of
    these methods to higher-resource languages. Finally, we outline ChatLSP, a
    conservative extension to the Language Server Protocol (LSP) that language
    servers can implement to expose capabilities that AI code completion
    systems of various designs can use to incorporate static context when
    generating prompts for an LLM.
